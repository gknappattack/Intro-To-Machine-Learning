{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVL7_bgmIAPR"
      },
      "source": [
        "# K-Nearest Neighbor Lab\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6ZbYjZZZ_yLV"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io.arff import loadarff\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCcEPx5VIORj"
      },
      "source": [
        "## 1. (40%) Correctly implement the k-nearest neighbor (KNN) algorithm and the KNN regression algorithm\n",
        "\n",
        "### Code requirements\n",
        "- Use Euclidean distance to decide closest neighbors. \n",
        "- Include optional distance weighting for both algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "_a2KSZ_7AN0G"
      },
      "outputs": [],
      "source": [
        "class KNNClassifier(BaseEstimator,ClassifierMixin):\n",
        "    def __init__(self, columntype=[], weight_type='inverse_distance'): ## add parameters here\n",
        "        self.columntype = columntype #Note This won't be needed until part 5\n",
        "        self.weight_type = weight_type\n",
        "        self.data = None\n",
        "        self.labels = None\n",
        "\n",
        "    def fit(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def predict(self, data, num_k):\n",
        "\n",
        "        k = num_k\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(len(data)):\n",
        "          curr_data = data[i]\n",
        "          distances = np.linalg.norm(self.data - curr_data, axis=1)\n",
        "\n",
        "          # Get indices of nearest neighbors and save labels\n",
        "          nearest_neighbor_ids = distances.argsort()[:k]\n",
        "          nearest_labels = [self.labels[id] for id in nearest_neighbor_ids]\n",
        "\n",
        "          # Sort alphabetically\n",
        "          nearest_labels = sorted(nearest_labels)\n",
        "\n",
        "          # Make prediction based on most common labels\n",
        "          c = Counter(nearest_labels)\n",
        "          c.most_common(1)\n",
        "          prediction = c.most_common(1)[0][0]\n",
        "\n",
        "          if self.weight_type == 'inverse_distance':\n",
        "            # Get weight values\n",
        "            closest_weights = [distances[id] for id in nearest_neighbor_ids]\n",
        "\n",
        "            # Get all unique values from \n",
        "            unique_labels = np.unique(nearest_labels)\n",
        "            inversed_weights = [0.0 for i in range(len(unique_labels))]\n",
        "\n",
        "            for i, label in enumerate(unique_labels):\n",
        "              curr_sum = 0\n",
        "              for index, value in enumerate(nearest_labels):\n",
        "                if value == label:\n",
        "                  curr_sum += 1 / closest_weights[index]\n",
        "              \n",
        "              inversed_weights[i] = curr_sum\n",
        "\n",
        "            \n",
        "            max_inverse_distance = np.argmax(inversed_weights)\n",
        "            predictions.append(unique_labels[max_inverse_distance])\n",
        "\n",
        "          else:\n",
        "            # Append prediction for no-weighting\n",
        "            predictions.append(prediction)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    # Regression variation of \n",
        "    def predict_regression(self, data, num_k):\n",
        "\n",
        "        k = num_k\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(len(data)):\n",
        "          curr_data = data[i]\n",
        "          distances = np.linalg.norm(self.data - curr_data, axis=1)\n",
        "\n",
        "          # Get indices of nearest neighbors and save labels\n",
        "          nearest_neighbor_ids = distances.argsort()[:k]\n",
        "          nearest_labels = [self.labels[id] for id in nearest_neighbor_ids]\n",
        "          prediction = 0\n",
        "\n",
        "          if self.weight_type == 'inverse_distance':\n",
        "            # Get weight values\n",
        "            closest_weights = [distances[id] for id in nearest_neighbor_ids]\n",
        "\n",
        "            numerator = 0\n",
        "            denominator = 0\n",
        "            for i in range(len(closest_weights)):\n",
        "              distance = closest_weights[i]\n",
        "              regression_label = nearest_labels[i]\n",
        "\n",
        "              numerator += (1/(distance**2)) * regression_label\n",
        "              denominator += 1 /(distance**2)\n",
        "\n",
        "            regression = numerator / denominator\n",
        "\n",
        "            predictions.append(regression)\n",
        "\n",
        "          else:\n",
        "            # Regression formula for no-weighting\n",
        "            sum = np.sum(nearest_labels)\n",
        "\n",
        "            regression = sum / k\n",
        "\n",
        "            # Append prediction for no-weighting\n",
        "            predictions.append(regression)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def predict_distance_metric(self, data, num_k):\n",
        "      k = num_k\n",
        "      predictions = []\n",
        "\n",
        "      for i in range(len(data)):\n",
        "        # Get distances using distance metric\n",
        "        distances = self.distance_metric(data[i])\n",
        "\n",
        "        nearest_neighbor_ids = distances.argsort()[:k]\n",
        "        nearest_labels = [self.labels[id] for id in nearest_neighbor_ids]\n",
        "\n",
        "        # Sort alphabetically\n",
        "        nearest_labels = sorted(nearest_labels)\n",
        "\n",
        "        # Make prediction based on most common labels\n",
        "        c = Counter(nearest_labels)\n",
        "        c.most_common(1)\n",
        "        prediction = c.most_common(1)[0][0]\n",
        "\n",
        "        if self.weight_type == 'inverse_distance':\n",
        "          # Get weight values\n",
        "          closest_weights = [distances[id] for id in nearest_neighbor_ids]\n",
        "\n",
        "          # Get all unique values from \n",
        "          unique_labels = np.unique(nearest_labels)\n",
        "          inversed_weights = [0.0 for i in range(len(unique_labels))]\n",
        "\n",
        "          for i, label in enumerate(unique_labels):\n",
        "            curr_sum = 0\n",
        "            for index, value in enumerate(nearest_labels):\n",
        "              if value == label:\n",
        "                curr_sum += 1 / closest_weights[index]\n",
        "            \n",
        "            inversed_weights[i] = curr_sum\n",
        "\n",
        "          max_inverse_distance = np.argmax(inversed_weights)\n",
        "\n",
        "          # Append prediction using inverse weighting\n",
        "          predictions.append(unique_labels[max_inverse_distance])\n",
        "\n",
        "        else:\n",
        "          # Append prediction for no-weighting\n",
        "          predictions.append(prediction)\n",
        "\n",
        "      return predictions\n",
        "    \n",
        "    def distance_metric(self, row_to_predict):\n",
        "      distances = []\n",
        "\n",
        "      # Iterate through every row in training data\n",
        "      for i, row in enumerate(self.data):\n",
        "\n",
        "        test_values = []\n",
        "        train_values = []\n",
        "\n",
        "        # Iterate through every column in train/test data\n",
        "        for j, col in enumerate(self.data[i]):\n",
        "          data_type = self.columntype[j]\n",
        "\n",
        "          # Check if values are equal for nomial data, if so, replace values with 0, else 1\n",
        "          if data_type == \"nominal\":\n",
        "            if row_to_predict[j] == self.data[i][j]:\n",
        "              test_values.append(0.0)\n",
        "              train_values.append(0.0)\n",
        "            else: # Values are different, distance will = 1 when subtracted\n",
        "              test_values.append(0.0)\n",
        "              train_values.append(1.0)\n",
        "          # Keep values as is for continuous data\n",
        "          else:\n",
        "            test_values.append(row_to_predict[j])\n",
        "            train_values.append(self.data[i][j])\n",
        "        \n",
        "        # Subtract values of both rows, get distance with linalg.norm\n",
        "        subtracted = np.array(train_values) - np.array(test_values)\n",
        "        distance = np.linalg.norm(subtracted)\n",
        "\n",
        "        # Get distance for current row and test data\n",
        "        distances.append(distance)\n",
        "\n",
        "      return np.array(distances)\n",
        "\n",
        "    #Returns the Mean score given input data and labels\n",
        "    def score(self, X, y):\n",
        "        total_values = 0\n",
        "        total_correct = 0\n",
        "\n",
        "        for i in range(len(y)):\n",
        "          predicted = X[i]\n",
        "          actual = y[i]\n",
        "\n",
        "          if predicted == actual:\n",
        "            total_correct += 1\n",
        "          total_values += 1\n",
        "\n",
        "        return total_correct/total_values\n",
        "    \n",
        "    def MSE(self, X, y):\n",
        "      means_squared = 0\n",
        "      for i in range(len(y)):\n",
        "        means_squared += (y[i] - X[i])**2\n",
        "      return means_squared / len(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF-8zEaOG5Ah"
      },
      "source": [
        "## 1.1 Debug and Evaluation\n",
        "\n",
        "Debug and Evaluate your model using the parameters below:\n",
        "\n",
        "- Use distance weighting\n",
        "- KNN = 3 (three nearest neighbors)\n",
        "- Don’t normalize the data\n",
        "- Use Euclidean Distance\n",
        "\n",
        "---\n",
        "\n",
        "### 1.1.1 Debug\n",
        "\n",
        "- Use this [glass training set](https://byu.instructure.com/courses/14142/files?preview=4660939) and this [glass test set](https://byu.instructure.com/courses/14142/files?preview=4660941)\n",
        "- Use distance weighting\n",
        "- KNN = 3 (three nearest neighbors)\n",
        "- Don’t normalize the data\n",
        "- Use Euclidean Distance\n",
        "\n",
        "Expected Results:\n",
        "- Not using inverse weighted distancing = roughly [68.29%]\n",
        "- Link to [glass no_inverse debug solution](https://byu.instructure.com/courses/14142/files?preview=4660947)\n",
        "\n",
        "- Using inverse weighted distancing = roughly [74.39%]\n",
        "- Link to [glass inverse debug solution](https://byu.instructure.com/courses/14142/files?preview=4660954)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLAh1uD4G5Ai",
        "outputId": "037afa28-2f00-44dc-9e52-72adf5b09a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model accuracy on Glass Dataset No Inverse\n",
            "0.6829268292682927\n",
            "\n",
            "Model accuracy on Glass Dataset With Inverse\n",
            "0.7073170731707317\n"
          ]
        }
      ],
      "source": [
        "# Load glass data\n",
        "# Load debug training data \n",
        "raw_data = loadarff('glass_train.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "values = df.drop('Type',axis=1)\n",
        "labels = df['Type']\n",
        "labels = labels.str.decode(\"utf-8\")\n",
        "1\n",
        "X_train = values.values\n",
        "y_train = labels.values\n",
        "\n",
        "\n",
        "# Train on training set\n",
        "model = KNNClassifier(weight_type=\"none\")\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "# Load test set\n",
        "raw_data = loadarff('glass_test.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "values = df.drop('Type',axis=1)\n",
        "labels = df['Type']\n",
        "labels = labels.str.decode(\"utf-8\")\n",
        "\n",
        "X_test = values.values\n",
        "y_test = labels.values\n",
        "\n",
        "# Predict on test set\n",
        "print(\"\\nModel accuracy on Glass Dataset No Inverse\")\n",
        "print(model.score(model.predict(X_test, 3), y_test))\n",
        "\n",
        "\n",
        "# Predict model using inverse weights\n",
        "model = KNNClassifier()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "print(\"\\nModel accuracy on Glass Dataset With Inverse\")\n",
        "print(model.score(model.predict(X_test, 3), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfI2DlecG5Ai"
      },
      "source": [
        "### 1.1.2 Evaluate\n",
        "\n",
        "We will evaluate your model based on its performance on the [diabetes](https://archive.ics.uci.edu/ml/datasets/Diabetes) problem.\n",
        "- Use this [diabetes training set](https://byu.instructure.com/courses/14142/files?preview=4660977) and this [diabetes test set](https://byu.instructure.com/courses/14142/files?preview=4660978)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvJH1lfdG5Ai",
        "outputId": "01b9223e-a9c2-4e41-970a-4fbd2014553b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model accuracy on Diabetes data No Inverse\n",
            "0.8411458333333334\n",
            "\n",
            "Model accuracy on Diabetes data With Inverse: \n",
            "0.8203125\n"
          ]
        }
      ],
      "source": [
        "# Load diabetes data\n",
        "# Load debug training data \n",
        "raw_data = loadarff('diabetes_train.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "values = df.drop('class',axis=1)\n",
        "labels = df['class']\n",
        "labels = labels.str.decode(\"utf-8\")\n",
        "1\n",
        "X_train = values.values\n",
        "y_train = labels.values\n",
        "\n",
        "\n",
        "# Train on training set\n",
        "model = KNNClassifier(weight_type=\"none\")\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "# Load test set\n",
        "raw_data = loadarff('diabetes_test.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "values = df.drop('class',axis=1)\n",
        "labels = df['class']\n",
        "labels = labels.str.decode(\"utf-8\")\n",
        "\n",
        "X_test = values.values\n",
        "y_test = labels.values\n",
        "\n",
        "# Predict on test set\n",
        "print(\"\\nModel accuracy on Diabetes data No Inverse\")\n",
        "print(model.score(model.predict(X_test, 3), y_test))\n",
        "\n",
        "# Predict model using inverse weights\n",
        "model = KNNClassifier()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "print(\"\\nModel accuracy on Diabetes data With Inverse: \")\n",
        "print(model.score(model.predict(X_test, 3), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vWiTdlbR2Xh"
      },
      "source": [
        "## 2. (10%) Use the k-nearest neighbor algorithm (without distance weighting) for the [magic telescope](http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope) problem\n",
        "\n",
        "- Use this [magic telescope training set](https://byu.instructure.com/courses/14142/files?preview=4660988) and this [magic telescope test set](https://byu.instructure.com/courses/14142/files?preview=4660989) \n",
        "\n",
        "### 2.1\n",
        "- Try it with k=3 and without normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "4SSoasDQSKXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91fe2d8c-bf2b-493d-b08a-dfa25f828836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model accuracy on Magic Telescope No Normalization, no Inverse Weighting\n",
            "0.8082808280828083\n"
          ]
        }
      ],
      "source": [
        "# Load magic telescope data\n",
        "# Load diabetes data\n",
        "# Load debug training data \n",
        "raw_data = loadarff('magic_telescope_train.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "\n",
        "values = df.drop('class',axis=1)\n",
        "labels = df['class']\n",
        "labels = labels.str.decode(\"utf-8\")\n",
        "\n",
        "\n",
        "X_train = values.values\n",
        "y_train = labels.values\n",
        "\n",
        "\n",
        "# Train on training set\n",
        "model = KNNClassifier(weight_type=\"none\")\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "# Load test set\n",
        "raw_data = loadarff('magic_telescope_test.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "values = df.drop('class',axis=1)\n",
        "labels = df['class']\n",
        "labels = labels.str.decode(\"utf-8\")\n",
        "\n",
        "X_test = values.values\n",
        "y_test = labels.values\n",
        "\n",
        "# Predict on test set\n",
        "print(\"\\nModel accuracy on Magic Telescope No Normalization, no Inverse Weighting\")\n",
        "print(model.score(model.predict(X_test, 3), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M63Rc2ieG5Aj"
      },
      "source": [
        "### 2.2\n",
        "- Try it with k=3 and with normalization (input features normalized between 0 and 1). Use the normalization formula (x-xmin)/(xmax-xmin)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = loadarff('magic_telescope_train.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qfv1WAuYYLum",
        "outputId": "b3c79255-10d2-48de-fcd8-f4699da495d3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   fLength   fWidth   fSize   fConc  fConc1    fAsym  fM3Long  fM3Trans  \\\n",
              "0  22.7815  14.9526  2.4362  0.4982  0.2509 -14.2836  -9.3635   13.0939   \n",
              "1  40.6756  15.5940  2.7447  0.2772  0.1449  19.6226   9.0297    7.4157   \n",
              "2  12.8427  11.3821  2.1255  0.7191  0.4232 -14.9637   8.5891    6.9418   \n",
              "3  77.0262  32.2880  3.3502  0.3152  0.1868 -28.6712 -35.9464  -31.4001   \n",
              "4  27.5146  11.2114  2.6637  0.4643  0.2687  42.4467  33.0422   -6.3980   \n",
              "\n",
              "    fAlpha     fDist class  \n",
              "0   3.0779  141.5620  b'g'  \n",
              "1  15.4260  193.2340  b'g'  \n",
              "2  82.4198  183.9790  b'g'  \n",
              "3   0.8940  357.0440  b'g'  \n",
              "4  50.3004  239.8878  b'h'  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-135d33b2-793a-4916-b09f-a81d2846fb23\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fLength</th>\n",
              "      <th>fWidth</th>\n",
              "      <th>fSize</th>\n",
              "      <th>fConc</th>\n",
              "      <th>fConc1</th>\n",
              "      <th>fAsym</th>\n",
              "      <th>fM3Long</th>\n",
              "      <th>fM3Trans</th>\n",
              "      <th>fAlpha</th>\n",
              "      <th>fDist</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22.7815</td>\n",
              "      <td>14.9526</td>\n",
              "      <td>2.4362</td>\n",
              "      <td>0.4982</td>\n",
              "      <td>0.2509</td>\n",
              "      <td>-14.2836</td>\n",
              "      <td>-9.3635</td>\n",
              "      <td>13.0939</td>\n",
              "      <td>3.0779</td>\n",
              "      <td>141.5620</td>\n",
              "      <td>b'g'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40.6756</td>\n",
              "      <td>15.5940</td>\n",
              "      <td>2.7447</td>\n",
              "      <td>0.2772</td>\n",
              "      <td>0.1449</td>\n",
              "      <td>19.6226</td>\n",
              "      <td>9.0297</td>\n",
              "      <td>7.4157</td>\n",
              "      <td>15.4260</td>\n",
              "      <td>193.2340</td>\n",
              "      <td>b'g'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12.8427</td>\n",
              "      <td>11.3821</td>\n",
              "      <td>2.1255</td>\n",
              "      <td>0.7191</td>\n",
              "      <td>0.4232</td>\n",
              "      <td>-14.9637</td>\n",
              "      <td>8.5891</td>\n",
              "      <td>6.9418</td>\n",
              "      <td>82.4198</td>\n",
              "      <td>183.9790</td>\n",
              "      <td>b'g'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>77.0262</td>\n",
              "      <td>32.2880</td>\n",
              "      <td>3.3502</td>\n",
              "      <td>0.3152</td>\n",
              "      <td>0.1868</td>\n",
              "      <td>-28.6712</td>\n",
              "      <td>-35.9464</td>\n",
              "      <td>-31.4001</td>\n",
              "      <td>0.8940</td>\n",
              "      <td>357.0440</td>\n",
              "      <td>b'g'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27.5146</td>\n",
              "      <td>11.2114</td>\n",
              "      <td>2.6637</td>\n",
              "      <td>0.4643</td>\n",
              "      <td>0.2687</td>\n",
              "      <td>42.4467</td>\n",
              "      <td>33.0422</td>\n",
              "      <td>-6.3980</td>\n",
              "      <td>50.3004</td>\n",
              "      <td>239.8878</td>\n",
              "      <td>b'h'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-135d33b2-793a-4916-b09f-a81d2846fb23')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-135d33b2-793a-4916-b09f-a81d2846fb23 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-135d33b2-793a-4916-b09f-a81d2846fb23');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "pcYOz8GhG5Aj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de3b72e-763e-43b9-c7e0-14e5331d1319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model accuracy on Magic Telescope With Normalization, no Inverse Weighting\n",
            "0.8157815781578158\n"
          ]
        }
      ],
      "source": [
        "# Train/Predict with normalization\n",
        "# Load magic telescope data\n",
        "raw_data = loadarff('magic_telescope_train.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "\n",
        "values = df.drop('class',axis=1)\n",
        "\n",
        "labels = df['class']\n",
        "labels = labels.str.decode(\"utf-8\")\n",
        "\n",
        "# Normalize training data\n",
        "values = (values - values.min())/(values.max() - values.min())\n",
        "\n",
        "X_train = values.values\n",
        "y_train = labels.values\n",
        "\n",
        "\n",
        "# Train on training set\n",
        "model = KNNClassifier(weight_type=\"none\")\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "# Load test set\n",
        "raw_data = loadarff('magic_telescope_test.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "values = df.drop('class',axis=1)\n",
        "labels = df['class']\n",
        "labels = labels.str.decode(\"utf-8\")\n",
        "\n",
        "# Normalize test data\n",
        "values = (values - values.min())/(values.max() - values.min())\n",
        "\n",
        "# Turn to numpy arrays\n",
        "X_test = values.values\n",
        "y_test = labels.values\n",
        "\n",
        "# Predict on test set\n",
        "print(\"\\nModel accuracy on Magic Telescope With Normalization, no Inverse Weighting\")\n",
        "print(model.score(model.predict(X_test, 3), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prNcYRSqG5Aj"
      },
      "source": [
        "On the magic telescope dataset, there was a small but noticable increase in prediction accuracy when using the normalized dataset. Without normalized data, the accuracy was 80.8% but with the normalized data, accuracy was 81.6%, an increase of 0.8%. Even though that is a small increase, in a classification task, any gain is valuable. \n",
        "\n",
        "Normalizing data is valuable especially when using a dataset with a wider variety of numeric features. In the magic telescope data, the first column, fLength features values of various lengths. Even in just the first few rows, values range from the 20's to the 70's, creating a big disparity in terms of distance between data points. Other columns such as the fConc1 are values between 0 and 1, which have a very small range of distances between points. When we calculate distance between a test point and the training data, if the data is not normalized, then the model will tend to rely on the data features with the largest absolute values since those influence the total distances the most. If the classification does correlate well with those values, then the prediction accuracy is usually passable, however, in cases where features with large absolute values does not work well to predict the output class, unnormalized data will have inconsistent results in prediction. By scaling the data with normalization, the general relationship between the points and classes is maintained, but the especially large and small outlier values will not affect the predictions as heavily since they are all contained within the same 0 to 1 range. \n",
        "\n",
        "This effect is seen in our magic telescope dataset, as we did have the aforementioned increase in accuracy when using the normalized dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um-ukzkGG5Ak"
      },
      "source": [
        "### 2.3\n",
        "\n",
        "- Using your normalized data, create one graph with classification accuracy on the test set over k values. \n",
        "    - Use odd values of k from 1 to 15.\n",
        "- As a rough sanity check, typical knn accuracies for the magic telescope data set are 75-85%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "4FfHGr5cG5Ak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e3922a-33fb-4e7a-8123-bc55c0f190fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model accuracy on Magic Telescope With Normalization, no Inverse Weighting for k=1,3..,15\n",
            "K =  1\n",
            "0.7874287428742874\n",
            "K =  3\n",
            "0.8157815781578158\n",
            "K =  5\n",
            "0.8271827182718272\n",
            "K =  7\n",
            "0.8294329432943295\n",
            "K =  9\n",
            "0.8282328232823283\n",
            "K =  11\n",
            "0.8294329432943295\n",
            "K =  13\n",
            "0.8297329732973298\n",
            "K =  15\n",
            "0.8304830483048304\n"
          ]
        }
      ],
      "source": [
        "# Train/Predict with normalization using k=1,3,...,15\n",
        "# Load magic telescope data\n",
        "raw_data = loadarff('magic_telescope_train.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "\n",
        "values = df.drop('class',axis=1)\n",
        "\n",
        "labels = df['class']\n",
        "labels = labels.str.decode(\"utf-8\")\n",
        "\n",
        "# Normalize training data\n",
        "values = (values - values.min())/(values.max() - values.min())\n",
        "\n",
        "X_train = values.values\n",
        "y_train = labels.values\n",
        "\n",
        "\n",
        "# Train on training set\n",
        "model = KNNClassifier(weight_type=\"none\")\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "# Load test set\n",
        "raw_data = loadarff('magic_telescope_test.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "values = df.drop('class',axis=1)\n",
        "labels = df['class']\n",
        "labels = labels.str.decode(\"utf-8\")\n",
        "\n",
        "# Normalize test data\n",
        "values = (values - values.min())/(values.max() - values.min())\n",
        "\n",
        "X_test = values.values\n",
        "y_test = labels.values\n",
        "\n",
        "# Predict on test set\n",
        "print(\"\\nModel accuracy on Magic Telescope With Normalization, no Inverse Weighting for k=1,3..,15\")\n",
        "\n",
        "# Graph classification accuracy over k\n",
        "for k in range (1,17, 2):\n",
        "  print(\"K = \", k)\n",
        "  print(model.score(model.predict(X_test, k), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KofedJniG5Ak"
      },
      "source": [
        "# For the rest of the experiments use only normalized data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIRG42TgSR4x"
      },
      "source": [
        "## 3. (10%) Use the regression variation of your algorithm (without distance weighting) for the [housing price prediction](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) problem.\n",
        "\n",
        "- Use this [housing training set](https://byu.instructure.com/courses/14142/files?preview=4660994) and this [housing test set](https://byu.instructure.com/courses/14142/files?preview=4660995).\n",
        "- Use Mean Square Error (MSE) on the test set as your accuracy metric for this case.\n",
        "    - Do not normalize regression output values\n",
        "- Graph MSE on the test set with odd values of k from 1 to 15\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "KBGUn43ASiXW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "9d4a490e-b812-41b9-a4e7-9075f98b8222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model regression accuracy on Housing Data With Normalization, no Inverse Weighting for k=1,3...15\n",
            "K =  1\n",
            "23.580980392156874\n",
            "K =  3\n",
            "14.215446623093682\n",
            "K =  5\n",
            "16.31825882352942\n",
            "K =  7\n",
            "20.028679471788713\n",
            "K =  9\n",
            "20.14123456790124\n",
            "K =  11\n",
            "21.274496840058347\n",
            "K =  13\n",
            "22.408194686158495\n",
            "K =  15\n",
            "22.584964705882353\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Load housing price prediction data\n",
        "raw_data = loadarff('housing_train.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "\n",
        "values = df.drop('MEDV',axis=1)\n",
        "labels = df['MEDV']\n",
        "\n",
        "values['CHAS'] = values['CHAS'].str.decode('utf-8')\n",
        "values['CHAS'] = pd.to_numeric(values['CHAS'],downcast=\"float\")\n",
        "\n",
        "for column in values:\n",
        "  if values[column].dtype == \"float64\":\n",
        "    values[column] = (values[column] - values[column].min()) / (values[column].max() - values[column].min())\n",
        "\n",
        "# Normalize training data\n",
        "values = (values - values.min())/(values.max() - values.min())\n",
        "\n",
        "X_train = values.values\n",
        "y_train = labels.values\n",
        "\n",
        "# Train on training set\n",
        "model = KNNClassifier(weight_type=\"none\")\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "# Load test set\n",
        "raw_data = loadarff('housing_test.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "values = df.drop('MEDV',axis=1)\n",
        "labels = df['MEDV']\n",
        "\n",
        "values['CHAS'] = values['CHAS'].str.decode('utf-8')\n",
        "values['CHAS'] = pd.to_numeric(values['CHAS'],downcast=\"float\")\n",
        "\n",
        "for column in values:\n",
        "  if values[column].dtype == \"float64\":\n",
        "    values[column] = (values[column] - values[column].min()) / (values[column].max() - values[column].min())\n",
        "\n",
        "# Normalize test data\n",
        "values = (values - values.min())/(values.max() - values.min())\n",
        "\n",
        "# Turn to numpy arrays\n",
        "X_test = values.values\n",
        "y_test = labels.values\n",
        "\n",
        "# Predict on test set\n",
        "print(\"\\nModel regression accuracy on Housing Data With Normalization, no Inverse Weighting for k=1,3...15\")\n",
        "# Train/Predict using k=1,3,...,15\n",
        "k_vals = []\n",
        "MSE_vals = []\n",
        "for k in range (1,17, 2):\n",
        "  k_vals.append(k)\n",
        "  print(\"K = \", k)\n",
        "  curr_MSE = model.MSE(model.predict_regression(X_test, k), y_test)\n",
        "  print(curr_MSE)\n",
        "  MSE_vals.append(curr_MSE)\n",
        "\n",
        "\n",
        "# Graph MSE over k\n",
        "plt.plot(k_vals, MSE_vals, marker=\"o\")\n",
        "plt.xlabel(\"Value of k\")\n",
        "plt.ylabel(\"MSE Value\")\n",
        "plt.title(\"Model Means Squared Error by k Value\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v19fpixqTe-7"
      },
      "source": [
        "## 4. (15%) Repeat your experiments for magic telescope and housing using distance-weighted (inverse of distance squared) voting and discuss your results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZdbKthWG5Al"
      },
      "source": [
        "## 4.1 Magic Telescope Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "ZCPFUAGTS2sX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc79f27-b628-4d7c-faab-95d0074ce210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model accuracy on Magic Telescope With Normalization, with Inverse Weighting for k=1,3..,15\n",
            "K =  1\n",
            "0.7874287428742874\n",
            "K =  3\n",
            "0.8156315631563157\n",
            "K =  5\n",
            "0.8271827182718272\n",
            "K =  7\n",
            "0.8271827182718272\n",
            "K =  9\n",
            "0.8256825682568257\n",
            "K =  11\n",
            "0.8249324932493249\n",
            "K =  13\n",
            "0.8229822982298229\n",
            "K =  15\n",
            "0.8213321332133213\n"
          ]
        }
      ],
      "source": [
        "# Train/Predict magic telescope using distance-weighted voting\n",
        "# Load magic telescope data\n",
        "raw_data = loadarff('magic_telescope_train.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "\n",
        "values = df.drop('class',axis=1)\n",
        "\n",
        "labels = df['class']\n",
        "labels = labels.str.decode(\"utf-8\")\n",
        "\n",
        "# Normalize training data\n",
        "values = (values - values.min())/(values.max() - values.min())\n",
        "\n",
        "X_train = values.values\n",
        "y_train = labels.values\n",
        "\n",
        "\n",
        "# Train on training set\n",
        "model = KNNClassifier()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "# Load test set\n",
        "raw_data = loadarff('magic_telescope_test.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "values = df.drop('class',axis=1)\n",
        "labels = df['class']\n",
        "labels = labels.str.decode(\"utf-8\")\n",
        "\n",
        "# Normalize test data\n",
        "values = (values - values.min())/(values.max() - values.min())\n",
        "\n",
        "X_test = values.values\n",
        "y_test = labels.values\n",
        "\n",
        "# Predict on test set\n",
        "print(\"\\nModel accuracy on Magic Telescope With Normalization, with Inverse Weighting for k=1,3..,15\")\n",
        "\n",
        "# Graph classification accuracy over k\n",
        "for k in range (1,17, 2):\n",
        "  print(\"K = \", k)\n",
        "  print(model.score(model.predict(X_test, k), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrRubOCeG5Al"
      },
      "source": [
        "## 4.2 Housing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "VxY5ZC8qG5Al",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "c79b6e87-1c6c-4565-d362-047dfae10f67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model regression accuracy on Housing Data With Normalization, with Inverse Weighting for k=1,3...15\n",
            "K =  1\n",
            "23.58098039215687\n",
            "K =  3\n",
            "12.310558867751292\n",
            "K =  5\n",
            "11.179088494440384\n",
            "K =  7\n",
            "11.07389265789775\n",
            "K =  9\n",
            "11.736722469473246\n",
            "K =  11\n",
            "12.169079321844457\n",
            "K =  13\n",
            "12.598757816257994\n",
            "K =  15\n",
            "12.46086417012215\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Train/Predict housing using distance-weighted voting\n",
        "# Load housing price prediction data\n",
        "raw_data = loadarff('housing_train.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "\n",
        "values = df.drop('MEDV',axis=1)\n",
        "labels = df['MEDV']\n",
        "\n",
        "values['CHAS'] = values['CHAS'].str.decode('utf-8')\n",
        "values['CHAS'] = pd.to_numeric(values['CHAS'],downcast=\"float\")\n",
        "\n",
        "for column in values:\n",
        "  if values[column].dtype == \"float64\":\n",
        "    values[column] = (values[column] - values[column].min()) / (values[column].max() - values[column].min())\n",
        "\n",
        "# Normalize training data\n",
        "values = (values - values.min())/(values.max() - values.min())\n",
        "\n",
        "X_train = values.values\n",
        "y_train = labels.values\n",
        "\n",
        "# Train on training set\n",
        "model = KNNClassifier()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "# Load test set\n",
        "raw_data = loadarff('housing_test.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "values = df.drop('MEDV',axis=1)\n",
        "labels = df['MEDV']\n",
        "\n",
        "values['CHAS'] = values['CHAS'].str.decode('utf-8')\n",
        "values['CHAS'] = pd.to_numeric(values['CHAS'],downcast=\"float\")\n",
        "\n",
        "for column in values:\n",
        "  if values[column].dtype == \"float64\":\n",
        "    values[column] = (values[column] - values[column].min()) / (values[column].max() - values[column].min())\n",
        "\n",
        "# Normalize test data\n",
        "values = (values - values.min())/(values.max() - values.min())\n",
        "\n",
        "# Turn to numpy arrays\n",
        "X_test = values.values\n",
        "y_test = labels.values\n",
        "\n",
        "# Predict on test set\n",
        "print(\"\\nModel regression accuracy on Housing Data With Normalization, with Inverse Weighting for k=1,3...15\")\n",
        "# Train/Predict using k=1,3,...,15\n",
        "k_vals = []\n",
        "MSE_vals = []\n",
        "for k in range (1,17, 2):\n",
        "  k_vals.append(k)\n",
        "  print(\"K = \", k)\n",
        "  curr_MSE = model.MSE(model.predict_regression(X_test, k), y_test)\n",
        "  print(curr_MSE)\n",
        "  MSE_vals.append(curr_MSE)\n",
        "\n",
        "\n",
        "# Graph MSE over k\n",
        "plt.plot(k_vals, MSE_vals, marker=\"o\")\n",
        "plt.xlabel(\"Value of k\")\n",
        "plt.ylabel(\"MSE Value\")\n",
        "plt.title(\"Model Means Squared Error by k Value\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9dKL2YiG5Al"
      },
      "source": [
        "It is clear from just the comparison to previous tests that taking the steps of normalizing the data and running the distance weighting in our calculations improves the performance of both the Classification and the Regression of the model.\n",
        "\n",
        "With the classifier, the accuracy increase was quite significant. In part 2, when I ran the magic telescope dataset with the normalization but no distance weighting, the best accuracy I got (with k = 3) was 81.6%. In this part, I ran the tests with the odd k values again to get a comparison, and the best results (both k = 5 and k = 7) got as high as 82.7%, a 1.1% increase over the tests in part 2. A 1% gain still sounds small, but that is a significant jump, especially when the test data is multiple hundreds of examples. 1% gain indicates that multiple examples that were previously classified incorrectly were improved. I think the increase in the k value was the most significant factor in the improved performance, but the distance weighting still helped to perform a similar function to the data normalization and helping to appropriately reward closer distances, which becomes more obvious when more k values are included in the calculations. \n",
        "\n",
        "I think the affects of the inverse distance weighting with the normalization is more obvious in the regression function on the Housing data. In part 3, when I ran the regression, there was an initial high MSE for k=1 then it dropped down to about 14 and steadily increased as k increased. Logically thinking, this result makes sense; when there is only 1 k, the outcome is unlikely to be very accurate because there is no other neighbors to make a comparison with. As k gets very large, without distance weighting the MSE will start to scale regardless since more values which are progressively further away from the test point are added in to the calculations and their values are weighing heavier on the result since they are not scaled. That affect is nullified by implementing the inverse weighting for the regression prediction. The initial high MSE when k=1 is still there as expected, but after the drop off at k=3 the MSE stays relatively low. This makes sense, since the inverse weighting insures that even if more values are considered, they are being weighted appropriately according to their relative location and distance from the new point. \n",
        "\n",
        "In both cases of regression and classification there was a performance improvement by using data normalization and distance weighting together, but the improvement was most easily seen in the output of the regression model. The graphs help to visualize the improvement as k continues to increase for the Regressor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdB1qYlLG5Am"
      },
      "source": [
        "## 5. (10%) Use the k-nearest neighbor algorithm to solve the [credit-approval](https://archive.ics.uci.edu/ml/datasets/Credit+Approval) (credit-a) problem.\n",
        "\n",
        "- Use this [credit approval dataset](https://byu.instructure.com/courses/14142/files?preview=4660998)\n",
        "    - Use a 70/30 split of the data for the training/test set\n",
        "- Note that this set has both continuous and nominal attributes, together with don’t know values. \n",
        "- Implement and justify a distance metric which supports continuous, nominal, and don’t know attribute values\n",
        "    - You need to handle don't knows with the distance metric, not by imputing a value.\n",
        "    - More information on distance metrics can be found [here](https://www.jair.org/index.php/jair/article/view/10182/24168).\n",
        "- Use your own choice for k.\n",
        "- As a rough sanity check, typical knn accuracies for the credit data set are 70-80%.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "yKTBH0iKG5Am",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad60a1d3-a331-42b3-8604-6035c5b28e49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy on Credit Approval Dataset with No Inverse Weighting and Normalization and k =  5\n",
            "0.8067632850241546\n"
          ]
        }
      ],
      "source": [
        "# Load dataset and split into train/test sets\n",
        "# Train/Predict with normalization\n",
        "\n",
        "raw_data = loadarff('credit_approval.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "df_train, df_test = train_test_split(df, test_size=0.3)\n",
        "\n",
        "# Reset indicies to fix look up\n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "df_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Get column types for each column\n",
        "column_types = []\n",
        "\n",
        "# Decode byte strings to normal strings and normalize test data\n",
        "for column in df_train:\n",
        "  # Decode byte string for non-numeric data\n",
        "  if df_train[column].dtype != \"float64\":\n",
        "    column_types.append(\"nominal\")\n",
        "    df_train[column] = df_train[column].str.decode('utf-8')\n",
        "  else:\n",
        "    column_types.append(\"continuous\")\n",
        "    df_train[column] = (df_train[column] - df_train[column].min()) / (df_train[column].max() - df_train[column].min())\n",
        "\n",
        "# Separate training data to values and labels\n",
        "X_train = df_train.drop('class', axis=1).values\n",
        "y_train = df_train['class']\n",
        "\n",
        "# Initialize model\n",
        "model = KNNClassifier(columntype=column_types)\n",
        "\n",
        "# \"Fit\" model with training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Prepare test data for predicting\n",
        "# Decode byte strings to normal strings and normalize test data\n",
        "for column in df_test:\n",
        "  # Decode byte string for non-numeric data\n",
        "  if df_test[column].dtype != \"float64\":\n",
        "    df_test[column] = df_test[column].str.decode('utf-8')\n",
        "  else:\n",
        "    df_test[column] = (df_test[column] - df_test[column].min()) / (df_test[column].max() - df_test[column].min())\n",
        "\n",
        "# Separate training data to values and labels\n",
        "X_test = df_test.drop('class', axis=1).values\n",
        "y_test = df_test['class']\n",
        "\n",
        "\n",
        "# Train/Predict credit-approval\n",
        "k_val = 5\n",
        "print(\"Model accuracy on Credit Approval Dataset with No Inverse Weighting and Normalization and k = \", k_val)\n",
        "print(model.score(model.predict_distance_metric(X_test, 3), y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1RFPFSOG5Am"
      },
      "source": [
        "I used a distance metric that would look at normial data and continuous data separately. When a new point was looked at to be predicted, I would then take a row of the training data to compare and iterate over each value. Using the pre-prepared column of value types, I would check if the data was nominal or continuous. If the data was continous, I would keep those values exactly how they are. If the data was nominal, I would then check the if the two values are the same. If they were then I would replace the value in both data points with a 0 for both. If they were different, I would replace the value in the original data point with a 1 and a 0 in the new data point. By doing this, when I subtract the two data points before running the result thorugh np.linalg.norm(), the resulting distance for that dimension will be 1 since 1 - 0 = 1. For datapoints with matching values, the resulting distance in that feature dimension would be 0 (0-0=0). By doing this, there is just two possibilities for distance for norminal data values of either 0 or 1, and the total distance between the two points is easy to calcualte now using numpy since all of the data is translated into numeric values. \n",
        "\n",
        "This approach works fine because the new nominal distance values do not exceed the possible values for the normalized continous data which is all between 0 and 1. It does make the distance a bit more heavily affected by the nominal datapoints, since having 0 and 1's in those columns weighs more heavily when most of the data is just float values betwen 0 and 1. I worried that this might affect the prediction output negatively, but as my testing showed, the prediction accuracy remained within the expected ranges. It is a much simpler approach than trying to weight the applied distance based on the total number of classes in each column, which will do a better job of giving a more nuanced value of what the new weight should be instead of just a blanket 1 or 0, but my results seemed well enough that I did not feel I needed to go that far. \n",
        "\n",
        "The other plus to this method is that it allows for easy treatment of NaN and N/A values. In the nominal data, if a missing value is encountered, its easy to treat it like its own category and assign it the distance of a non-matching value. It requires no extra effort so it reduced the need to make guesses as to the possible value may be. \n",
        "\n",
        "There are many approaches as the extra reading indicated. I definitely think a more nuanced approach to the nominal data weighting would work better depending on the complexity and size of the dataset. However, in this case, the results showed that even just a simple treatment of the catagorical data values can still produce satisfactory results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBBmeNQ7jvcQ"
      },
      "source": [
        "## 6. (15%) Use the scikit's KNN Classifier on magic telescope and KNN Regressor on housing and compare your results.\n",
        "\n",
        "- Try out different hyperparameters to see how well you can do. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I exported the scoring code to a function, so I can just create multiple instances of the classifier with different parameters \n",
        "# and run them and compare outputs without making the code below cluttered.\n",
        "def score(X, y):\n",
        "  total_values = 0\n",
        "  total_correct = 0\n",
        "\n",
        "  for i in range(len(y_test)):\n",
        "    predicted = X[i]\n",
        "    actual = y[i]\n",
        "\n",
        "    if predicted == actual:\n",
        "      total_correct += 1\n",
        "    total_values += 1\n",
        "  return total_correct / total_values"
      ],
      "metadata": {
        "id": "PzsTKoYhG_Ur"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing KNN Classifier on Magic Telescope"
      ],
      "metadata": {
        "id": "5v96LFncICIf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "OFQv70W2VyqJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8011288-bcda-4a25-9554-4890fbaaf47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 1 nearest neighbors\n",
            "0.7874287428742874\n",
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 3 nearest neighbors\n",
            "0.8157815781578158\n",
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 5 nearest neighbors\n",
            "0.8271827182718272\n",
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 7 nearest neighbors\n",
            "0.8294329432943295\n",
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 9 nearest neighbors\n",
            "0.8282328232823283\n",
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 11 nearest neighbors\n",
            "0.8294329432943295\n",
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 13 nearest neighbors\n",
            "0.8297329732973298\n",
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 15 nearest neighbors\n",
            "0.8304830483048304\n",
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 3 nearest neighbors, weights=distance\n",
            "0.8157815781578158\n",
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 3 nearest neighbors, algorithm=ball_tree\n",
            "0.8157815781578158\n",
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 3 nearest neighbors, algorithm=kd_tree\n",
            "0.8157815781578158\n",
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 3 nearest neighbors, algorithm=brute\n",
            "0.8157815781578158\n",
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 3 nearest neighbors, p=1\n",
            "0.8225322532253225\n",
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 3 nearest neighbors, p=3\n",
            "0.8108310831083109\n",
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 3 nearest neighbors, p=5\n",
            "0.8051305130513051\n",
            "\n",
            "Scikit learn model accuracy on Magic Telescope With Normalization, 15 nearest neighbors, p=1, default other parameters\n",
            "0.8331833183318332\n"
          ]
        }
      ],
      "source": [
        "# Train/Predict magic telescope using scikit's KNN\n",
        "# Load magic telescope data\n",
        "raw_data = loadarff('magic_telescope_train.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "\n",
        "values = df.drop('class',axis=1)\n",
        "\n",
        "labels = df['class']\n",
        "labels = labels.str.decode(\"utf-8\")\n",
        "\n",
        "# Normalize training data\n",
        "values = (values - values.min())/(values.max() - values.min())\n",
        "\n",
        "X_train = values.values\n",
        "y_train = labels.values\n",
        "\n",
        "# Load test set\n",
        "raw_data = loadarff('magic_telescope_test.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "\n",
        "values = df.drop('class',axis=1)\n",
        "labels = df['class']\n",
        "labels = labels.str.decode(\"utf-8\")\n",
        "\n",
        "# Normalize test data\n",
        "values = (values - values.min())/(values.max() - values.min())\n",
        "\n",
        "# Turn to numpy arrays\n",
        "X_test = values.values\n",
        "y_test = labels.values\n",
        "\n",
        "# Use 3 as default k value for running parameter tests\n",
        "k_vals = 3\n",
        "\n",
        "# First, try a test with varying k-neighbor values\n",
        "for k in range(1, 17, 2):\n",
        "  neigh = KNeighborsClassifier(n_neighbors=k)\n",
        "  neigh.fit(X_train, y_train)\n",
        "\n",
        "  print(\"\\nScikit learn model accuracy on Magic Telescope With Normalization, {} nearest neighbors\".format(k))\n",
        "  print(score(neigh.predict(X_test), y_test))\n",
        "\n",
        "# Try changing some of the parameters\n",
        "\n",
        "# Change weights parameter,\n",
        "neigh = KNeighborsClassifier(n_neighbors=k_vals, weights='distance')\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Magic Telescope With Normalization, {} nearest neighbors, weights=distance\".format(k_vals))\n",
        "print(score(neigh.predict(X_test), y_test))\n",
        "\n",
        "# Change algorithms\n",
        "neigh = KNeighborsClassifier(n_neighbors=k_vals, algorithm=\"ball_tree\")\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Magic Telescope With Normalization, {} nearest neighbors, algorithm=ball_tree\".format(k_vals))\n",
        "print(score(neigh.predict(X_test), y_test))\n",
        "\n",
        "neigh = KNeighborsClassifier(n_neighbors=k_vals, algorithm=\"kd_tree\")\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Magic Telescope With Normalization, {} nearest neighbors, algorithm=kd_tree\".format(k_vals))\n",
        "print(score(neigh.predict(X_test), y_test))\n",
        "\n",
        "neigh = KNeighborsClassifier(n_neighbors=k_vals, algorithm=\"brute\")\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Magic Telescope With Normalization, {} nearest neighbors, algorithm=brute\".format(k_vals))\n",
        "print(score(neigh.predict(X_test), y_test))\n",
        "\n",
        "# Change p in Minkowski metric\n",
        "neigh = KNeighborsClassifier(n_neighbors=k_vals, p=1) # Manhattan Distance\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Magic Telescope With Normalization, {} nearest neighbors, p=1\".format(k_vals))\n",
        "print(score(neigh.predict(X_test), y_test))\n",
        "\n",
        "neigh = KNeighborsClassifier(n_neighbors=k_vals, p=3) # Minkowski\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Magic Telescope With Normalization, {} nearest neighbors, p=3\".format(k_vals))\n",
        "print(score(neigh.predict(X_test), y_test))\n",
        "\n",
        "neigh = KNeighborsClassifier(n_neighbors=k_vals, p=5) # Minkowski\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Magic Telescope With Normalization, {} nearest neighbors, p=5\".format(k_vals))\n",
        "print(score(neigh.predict(X_test), y_test))\n",
        "\n",
        "\n",
        "# Try combining some of the best performing attributes (p=1, weights=\"uniform\", k=15, algorithm=auto)\n",
        "neigh = KNeighborsClassifier(n_neighbors=15, p=1)\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Magic Telescope With Normalization, {} nearest neighbors, p=1, default other parameters\".format(15))\n",
        "print(score(neigh.predict(X_test), y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/Predict Housing using scikit's KNN Regression\n",
        "# Load housing price prediction data\n",
        "raw_data = loadarff('housing_train.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "\n",
        "values = df.drop('MEDV',axis=1)\n",
        "labels = df['MEDV']\n",
        "\n",
        "values['CHAS'] = values['CHAS'].str.decode('utf-8')\n",
        "values['CHAS'] = pd.to_numeric(values['CHAS'],downcast=\"float\")\n",
        "\n",
        "for column in values:\n",
        "  if values[column].dtype == \"float64\":\n",
        "    values[column] = (values[column] - values[column].min()) / (values[column].max() - values[column].min())\n",
        "\n",
        "# Normalize training data\n",
        "values = (values - values.min())/(values.max() - values.min())\n",
        "\n",
        "X_train = values.values\n",
        "y_train = labels.values\n",
        "\n",
        "# Load test set\n",
        "raw_data = loadarff('housing_test.arff')\n",
        "df = pd.DataFrame(raw_data[0])\n",
        "values = df.drop('MEDV',axis=1)\n",
        "labels = df['MEDV']\n",
        "\n",
        "values['CHAS'] = values['CHAS'].str.decode('utf-8')\n",
        "values['CHAS'] = pd.to_numeric(values['CHAS'],downcast=\"float\")\n",
        "\n",
        "for column in values:\n",
        "  if values[column].dtype == \"float64\":\n",
        "    values[column] = (values[column] - values[column].min()) / (values[column].max() - values[column].min())\n",
        "\n",
        "# Normalize test data\n",
        "values = (values - values.min())/(values.max() - values.min())\n",
        "\n",
        "# Turn to numpy arrays\n",
        "X_test = values.values\n",
        "y_test = labels.values\n",
        "\n",
        "# Predict on test set\n",
        "k_vals=3\n",
        "\n",
        "# Train/Predict using k=1,3,...,15\n",
        "for k in range (1,17, 2):\n",
        "  print(\"\\nSklearn KNN Regressor regression accuracy on Housing Data With Normalization for k = \", k)\n",
        "  neigh = KNeighborsRegressor(n_neighbors=k)\n",
        "  neigh.fit(X_train, y_train)\n",
        "  print(neigh.score(X_test, y_test))\n",
        "\n",
        "\n",
        "# Try changing some of the parameters\n",
        "\n",
        "# Change weights parameter,\n",
        "neigh = KNeighborsRegressor(n_neighbors=k_vals, weights='distance')\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Housing Data With Normalization, {} nearest neighbors, weights=distance\".format(k_vals))\n",
        "print(neigh.score(X_test, y_test))\n",
        "\n",
        "# Change algorithms\n",
        "neigh = KNeighborsRegressor(n_neighbors=k_vals, algorithm=\"ball_tree\")\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Housing Data With Normalization, {} nearest neighbors, algorithm=ball_tree\".format(k_vals))\n",
        "print(neigh.score(X_test, y_test))\n",
        "\n",
        "neigh = KNeighborsRegressor(n_neighbors=k_vals, algorithm=\"kd_tree\")\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Housing Data With Normalization, {} nearest neighbors, algorithm=kd_tree\".format(k_vals))\n",
        "print(neigh.score(X_test, y_test))\n",
        "\n",
        "neigh = KNeighborsRegressor(n_neighbors=k_vals, algorithm=\"brute\")\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Housing Data With Normalization, {} nearest neighbors, algorithm=brute\".format(k_vals))\n",
        "print(neigh.score(X_test, y_test))\n",
        "\n",
        "# Change p in Minkowski metric\n",
        "neigh = KNeighborsRegressor(n_neighbors=k_vals, p=1) # Manhattan Distance\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Housing Data With Normalization, {} nearest neighbors, p=1\".format(k_vals))\n",
        "print(neigh.score(X_test, y_test))\n",
        "\n",
        "neigh = KNeighborsRegressor(n_neighbors=k_vals, p=3) # Minkowski\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Housing Data With Normalization, {} nearest neighbors, p=3\".format(k_vals))\n",
        "print(neigh.score(X_test, y_test))\n",
        "\n",
        "neigh = KNeighborsRegressor(n_neighbors=k_vals, p=5) # Minkowski\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Housing Data With Normalization, {} nearest neighbors, p=5\".format(k_vals))\n",
        "print(neigh.score(X_test, y_test))\n",
        "\n",
        "\n",
        "# Try combining some of the best performing attributes (p=3, weights=\"distance\", k=3, weights=distance, )\n",
        "neigh = KNeighborsRegressor(n_neighbors=3, p=3, weights='distance') \n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nScikit learn model accuracy on Housing Data With Normalization, {} nearest neighbors, p=3, and weights=distance\".format(3))\n",
        "print(neigh.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxhk_5GvJbAh",
        "outputId": "81404f95-2ad2-4227-cdc5-e0390ed8b617"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sklearn KNN Regressor regression accuracy on Housing Data With Normalization for k =  1\n",
            "0.6923150048765766\n",
            "\n",
            "Sklearn KNN Regressor regression accuracy on Housing Data With Normalization for k =  3\n",
            "0.81451663365283\n",
            "\n",
            "Sklearn KNN Regressor regression accuracy on Housing Data With Normalization for k =  5\n",
            "0.7870791076943359\n",
            "\n",
            "Sklearn KNN Regressor regression accuracy on Housing Data With Normalization for k =  7\n",
            "0.7386654819637777\n",
            "\n",
            "Sklearn KNN Regressor regression accuracy on Housing Data With Normalization for k =  9\n",
            "0.7371968613372147\n",
            "\n",
            "Sklearn KNN Regressor regression accuracy on Housing Data With Normalization for k =  11\n",
            "0.7224100377665461\n",
            "\n",
            "Sklearn KNN Regressor regression accuracy on Housing Data With Normalization for k =  13\n",
            "0.7076175308203643\n",
            "\n",
            "Sklearn KNN Regressor regression accuracy on Housing Data With Normalization for k =  15\n",
            "0.7053110328821024\n",
            "\n",
            "Scikit learn model accuracy on Housing Data With Normalization, 3 nearest neighbors, weights=distance\n",
            "0.8312297109129108\n",
            "\n",
            "Scikit learn model accuracy on Housing Data With Normalization, 3 nearest neighbors, algorithm=ball_tree\n",
            "0.81451663365283\n",
            "\n",
            "Scikit learn model accuracy on Housing Data With Normalization, 3 nearest neighbors, algorithm=kd_tree\n",
            "0.81451663365283\n",
            "\n",
            "Scikit learn model accuracy on Housing Data With Normalization, 3 nearest neighbors, algorithm=brute\n",
            "0.81451663365283\n",
            "\n",
            "Scikit learn model accuracy on Housing Data With Normalization, 3 nearest neighbors, p=1\n",
            "0.8179105390072786\n",
            "\n",
            "Scikit learn model accuracy on Housing Data With Normalization, 3 nearest neighbors, p=3\n",
            "0.8611668420615577\n",
            "\n",
            "Scikit learn model accuracy on Housing Data With Normalization, 3 nearest neighbors, p=5\n",
            "0.849079376647661\n",
            "\n",
            "Scikit learn model accuracy on Housing Data With Normalization, 3 nearest neighbors, p=3, and weights=distance\n",
            "0.8789134299035589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqSFAXwlk3Ms"
      },
      "source": [
        "*Report your comparison*\n",
        "I was surprised at how well the Sklearn models (both the Classification and Regression) were able to do when I tinkered with the parameters a bit. On initial results, the output values were pretty similar to my model's.\n",
        "\n",
        "Results for tests with k=3 and default parameters between Sklearn and my model.\n",
        "\n",
        "My Model (Magic Telescope): 81.6%\n",
        "My Model (Housing): MSE = 14.6\n",
        "Sklearn (Magic Telescope): 81.6%\n",
        "Sklearn (Housing): 81.5%\n",
        "\n",
        "(The comparison between the Sklearn score and the MSE is not exactly the same, but uses a similar calculation as I read on the Sklearn documentation).\n",
        "\n",
        "As it is shown, on the magic telescope with the same default settings and k-neighbors the two models actually performed the same accuracy wise. The comparison metric of MSE and score from sklearn are not the same, but the results are similar for the Regression models as well, though this would indicate my model performed a little better at this state.\n",
        "\n",
        "From there, once I played with the parameters (the results of each tests is shown in the output in the cells above, along with the parameters changed each test) there were some clear ones that improved the performance. For the classification model, a higher k value and using Manhatten distance instead of Euclidean distance showed increases in accuracy when tested individually. For the regression model, Manhatten distance along with a smaller k value and setting the weights parameter to distance instead of uniform all showed small accuracy gains individually. What surprised me the most was that when I ran a test combining all of these \"optimal\" parameters, in both cases the final result was the highest scoring test of them all. \n",
        "\n",
        "Sklearn Classification Model on Magic Telescope (with optimal settings): 83.3%\n",
        "Sklearn Regression Model on Housing Data (with optimal settings): 87.9%\n",
        "\n",
        "Both of these results were better than any test I had run on the same datasets with any combination of settings on my model, so it is clear which is suprerior. \n",
        "\n",
        "The 2 most interesting takeaways from this experimentation are the following:\n",
        "\n",
        "1) Different tasks call for different parameters. While the Classification and Regression models are similar yet different models inherantly, they still performed relatively well with just the same default settings. Yet, as I found out, the settings the provided the best performance on each tasks were actually quite difference, most notably with the number of k neighbors (15 for the telescope, 3 for the housing). It just goes to show the value of doing parameter search each time a new task is handled instead of defaulting to what is provided since you never know how each parameter affects the outcome.\n",
        "\n",
        "2) Sometimes mixing and matching parameters does work. In previous labs, when I messed around with parameters, some of them helped to improve accuracy but often they did nothing or actually worsened the results. Occasionally I would have a positive result on a couple of things, but when those parameters are combined, the result is back to average or gets worse. This time however, I was so surprised to see that all of the best settings I found on individual tests actually worked together really well when combined. Each time, the parameters improved accuracy from the base test a little bit, but when combined, the accuracy improved by almost a sum of all the individual gains. It was suprising, and it is not often the case that that actually happens, but it was a pleasant surprise when working on this lab to see the combined improvement show up when the parameters were just put together.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "lab_4_knn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}